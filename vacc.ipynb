{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import open3d as o3d\n",
    "import open3d.ml as _ml3d\n",
    "import open3d.ml.torch as ml3d\n",
    "import open_clip\n",
    "gpu_device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open('outputs.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from scipy.spatial import KDTree\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "def visualize_points(pts, colors):\n",
    "\n",
    "    # reduce the size of the point cloud\n",
    "    if pts.shape[0] > 100000:\n",
    "        idx = np.random.choice(pts.shape[0], 100000, replace=False)\n",
    "        pts = pts[idx]\n",
    "        colors = colors[idx]\n",
    "\n",
    "    # normalize colors\n",
    "    colors = colors.astype(float) / 255.0\n",
    "\n",
    "    # remove outliers\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=100, std_ratio=2.0)\n",
    "\n",
    "    # compute camera orientation\n",
    "    pcd.estimate_normals(\n",
    "                search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.04, max_nn=30)\n",
    "            )\n",
    "    pcd.orient_normals_consistent_tangent_plane(100)\n",
    "    normals = np.asarray(pcd.normals)\n",
    "    average_normal = np.mean(np.asarray(normals), axis=0)\n",
    "    average_normal /= np.linalg.norm(average_normal)\n",
    "\n",
    "    elevation = np.arcsin(average_normal[2])\n",
    "    azimuth = np.arctan2(average_normal[1], average_normal[0])\n",
    "\n",
    "    elevation_deg = np.degrees(elevation)\n",
    "    azimuth_deg = np.degrees(azimuth)\n",
    "\n",
    "    # create figure for rendering\n",
    "    fig = plt.figure(figsize=(6, 6), dpi=160) # 160\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # Set the camera view\n",
    "    ax.view_init(elev=elevation_deg, azim=azimuth_deg) # +8.4 + 2.3 # +5.5, -4.4\n",
    "\n",
    "    filtered_pts = np.asarray(pcd.points)\n",
    "    filtered_colors = np.asarray(pcd.colors)\n",
    "\n",
    "    ax.scatter(\n",
    "        filtered_pts[:, 0],\n",
    "        filtered_pts[:, 1],\n",
    "        filtered_pts[:, 2],\n",
    "        c=filtered_colors,\n",
    "        s=1,  # type: ignore\n",
    "    )\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.savefig(\"temp.png\", transparent=True, format='png', bbox_inches='tight', pad_inches=-0.4)\n",
    "    plt.close()\n",
    "\n",
    "    img = Image.open(\"temp.png\")\n",
    "    img = np.array(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "# idx = np.random.choice(outputs['xyz_pts'].shape[0], 500000, replace=False)\n",
    "# idx = outputs['segmentation_pts']['bin']\n",
    "# image = visualize_points(outputs['xyz_pts'][idx], outputs['rgb_pts'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-31): 32 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-23): 24 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 1024)\n",
       "  (ln_final): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-H-14\", \"laion2b_s32b_b79k\", device=gpu_device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# get the scene overview\n",
    "scene_img = visualize_points(outputs['xyz_pts'], outputs['rgb_pts'])\n",
    "scene_img = cv2.resize(scene_img, (512, 512))\n",
    "_scene_img = preprocess(Image.fromarray(scene_img)).unsqueeze(0).cuda().float()\n",
    "imgfeat = model.encode_image(_scene_img)\n",
    "global_feat = imgfeat.half().cuda()\n",
    "global_feat = torch.nn.functional.normalize(global_feat, dim=-1)\n",
    "\n",
    "FEAT_DIM = global_feat.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the points up to save memory\n",
    "\n",
    "segmented = {}\n",
    "\n",
    "max_points = 5000000\n",
    "starting_points = outputs['xyz_pts'].shape[0]\n",
    "\n",
    "discarded = []\n",
    "\n",
    "for key in outputs['segmentation_pts']:\n",
    "\n",
    "    object_indices = np.where(outputs['segmentation_pts'][key] == True)[0]\n",
    "    num_to_keep = int((object_indices.shape[0] / starting_points) * max_points)\n",
    "\n",
    "    indices_to_keep = np.random.choice(object_indices.shape[0], num_to_keep, replace=False)\n",
    "    reduced_object_indices = object_indices[indices_to_keep]\n",
    "\n",
    "    segmented[key] = {\n",
    "        \"points\": outputs['xyz_pts'][reduced_object_indices],\n",
    "        \"colors\": outputs['rgb_pts'][reduced_object_indices],\n",
    "    }\n",
    "\n",
    "    discarded.extend(object_indices[~np.isin(object_indices, reduced_object_indices)])\n",
    "\n",
    "\n",
    "# handle the unsegmented points\n",
    "outputs['xyz_pts'] = np.delete(outputs['xyz_pts'], discarded, axis=0)\n",
    "outputs['rgb_pts'] = np.delete(outputs['rgb_pts'], discarded, axis=0)\n",
    "\n",
    "num_to_keep = int((outputs['xyz_pts'].shape[0] / starting_points) * max_points)\n",
    "indices_to_keep = np.random.choice(outputs['xyz_pts'].shape[0], num_to_keep, replace=False)\n",
    "\n",
    "segmented['unsegmented'] = {\n",
    "    \"points\": outputs['xyz_pts'][indices_to_keep],\n",
    "    \"colors\": outputs['rgb_pts'][indices_to_keep],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the per-object features\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "feat_per_obj = []\n",
    "obj_sim_per_unit_area = []\n",
    "views = []\n",
    "for key in segmented:\n",
    "    obj_img = visualize_points(segmented[key]['points'], segmented[key]['colors'])\n",
    "    obj_img = cv2.resize(obj_img, (512, 512))\n",
    "    _obj_img= preprocess(Image.fromarray(obj_img)).unsqueeze(0).cuda().float()\n",
    "    obj_feat = model.encode_image(_obj_img).half().cuda()\n",
    "    feat_per_obj.append(obj_feat)\n",
    "\n",
    "    # calculate the cosine similarity between the global feature vector and the feature vector for the object and save that as well\n",
    "    _sim = cosine_similarity(global_feat, obj_feat)\n",
    "    obj_sim_per_unit_area.append(_sim)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # save the key so we know the order in which these were processed\n",
    "    views.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inter-object similarity to determine the relevance of each\n",
    "\n",
    "scores = torch.cat(obj_sim_per_unit_area).to(gpu_device)\n",
    "feat_per_obj = torch.cat(feat_per_obj, dim=0).to(gpu_device)\n",
    "\n",
    "# get the cosine simixlarity between the features of each object. This will be a square matrix where the (i, j)th entry is the cosine similarity between the ith and jth objects\n",
    "mask_sim_mat = torch.nn.functional.cosine_similarity(\n",
    "    feat_per_obj[:, :, None], feat_per_obj.t()[None, :, :]\n",
    ")\n",
    "mask_sim_mat_untouched = mask_sim_mat.clone()\n",
    "mask_sim_mat.fill_diagonal_(0.0) # set the diagonal to 0 because we don't want to consider the similarity between the same object\n",
    "mask_sim_mat = mask_sim_mat.mean(1)  # avg sim of each mask with each other mask\n",
    "softmax_scores = scores.cuda() - mask_sim_mat # subtracting the object-object relevance (which can be thought of as the relevance of the object in context of the other objects) object-scene similarity (which is kind of like global relevance) gives how much more or less important that object is than all the other objects\n",
    "softmax_scores = torch.nn.functional.softmax(softmax_scores, dim=0) # apply softmax to get the final scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain pixel aligned features\n",
    "\n",
    "for objidx in range(len(views)):\n",
    "    _weighted_feat = (\n",
    "        softmax_scores[objidx] * global_feat + (1 - softmax_scores[objidx]) * feat_per_obj[objidx]\n",
    "    )\n",
    "    _weighted_feat = torch.nn.functional.normalize(_weighted_feat, dim=-1).half().cpu().numpy()\n",
    "    repeated = np.tile(_weighted_feat, (segmented[views[objidx]]['points'].shape[0], 1))\n",
    "    segmented[views[objidx]]['features'] = repeated\n",
    "    \n",
    "# stack up the point cloud\n",
    "points = np.concatenate([segmented[key]['points'] for key in segmented], axis=0)\n",
    "colors = np.concatenate([segmented[key]['colors'] for key in segmented], axis=0)\n",
    "features = np.concatenate([segmented[key]['features'] for key in segmented], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5489779, 3)\n",
      "(5489779, 3)\n",
      "(5489779, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(points.shape)\n",
    "print(colors.shape)\n",
    "print(features.shape)\n",
    "np.save(\"points.npy\", points)\n",
    "np.save(\"colors.npy\", colors)\n",
    "np.save(\"features.npy\", features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed voxel aggregation with 55810 points\n",
      "Completed voxel aggregation with 19208 points\n",
      "Completed voxel aggregation with 19208 points\n"
     ]
    }
   ],
   "source": [
    "import open3d.ml.torch as ml3d\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import numpy as np\n",
    "\n",
    "points = np.load(\"points.npy\")\n",
    "features = np.load(\"features.npy\")\n",
    "\n",
    "# downsample further\n",
    "idx = np.random.choice(points.shape[0], 500000, replace=False)\n",
    "points = points[idx]\n",
    "features = features[idx]\n",
    "\n",
    "# voxel aggregation to blend features\n",
    "points = torch.tensor(points).float().cpu()\n",
    "features = torch.tensor(features).float().cpu()\n",
    "\n",
    "i = 0\n",
    "agg_rate = 0.005\n",
    "while points.shape[0] > 20000:\n",
    "    points, features = ml3d.ops.voxel_pooling(points, features, agg_rate, position_fn='nearest_neighbor', feature_fn='nearest_neighbor')\n",
    "    i += 1\n",
    "\n",
    "    print(f\"Completed voxel aggregation with {points.shape[0]} points\")\n",
    "\n",
    "    agg_rate += 0.005\n",
    "\n",
    "print(f\"Completed voxel aggregation with {points.shape[0]} points\")\n",
    "\n",
    "# save the point cloud\n",
    "torch.save(features, \"clip_feaures.pt\")\n",
    "torch.save(points, \"xyz_pts.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conceptfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
