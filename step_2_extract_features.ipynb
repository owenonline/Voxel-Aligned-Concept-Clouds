{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import open_clip\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "import psutil\n",
    "from lavis.models.eva_vit import create_eva_vit_g\n",
    "from lavis.common.registry import registry\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import cv2\n",
    "cpu_device = torch.device('cpu')\n",
    "gpu_device = torch.device('cuda')\n",
    "\n",
    "# Use 3dllm env for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mem_stats():\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_system_memory = mem.total / (1024 ** 2)\n",
    "    used_system_memory = mem.used / (1024 ** 2)\n",
    "    total_gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
    "    reserved_gpu_memory = torch.cuda.memory_reserved(0) / (1024 ** 2)\n",
    "    allocated_gpu_memory = torch.cuda.memory_allocated(0) / (1024 ** 2)\n",
    "    percent_gpu_total = (allocated_gpu_memory / total_gpu_memory)*100\n",
    "    percent_gpu_reserved = (reserved_gpu_memory / total_gpu_memory)*100\n",
    "    percent_cpu_total = (used_system_memory / total_system_memory)*100\n",
    "    print(f\"mem used gpu: {allocated_gpu_memory:.2f} MB, reserved gpu: {reserved_gpu_memory:.2f}MB -> {percent_gpu_total:.2f}% of total, {percent_gpu_reserved:.2f}% reserved\")\n",
    "    print(f\"mem used cpu: {used_system_memory:.2f} MB -> {percent_cpu_total:.2f}% of total\")\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# use the outputs from step 1\n",
    "with open('outputs.pkl', 'rb') as f:\n",
    "    outputs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "visual_encoder_start_time = time()\n",
    "\n",
    "visual_encoder = create_eva_vit_g(512, precision='fp32').to(gpu_device)\n",
    "\n",
    "visual_encoder_end_time = time()\n",
    "print(f\"Visual encoder took {visual_encoder_end_time - visual_encoder_start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outputs[\"sceneshotcam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sim_time = time()\n",
    "\n",
    "cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "# calculate the global feature vector\n",
    "scene_img = outputs[\"sceneshotcam\"][0] # rgb image\n",
    "print(scene_img.shape)\n",
    "scene_tensor = torch.tensor(scene_img).permute(2, 0, 1).unsqueeze(0).float().to(gpu_device)\n",
    "\n",
    "print(scene_tensor.shape)\n",
    "global_feat = visual_encoder(scene_tensor)\n",
    "global_feat = global_feat.half()\n",
    "global_feat = global_feat.mean(1)\n",
    "global_feat = torch.nn.functional.normalize(global_feat, dim=-1)\n",
    "FEAT_DIM = global_feat.shape[-1]\n",
    "\n",
    "global_sim_end_time = time()\n",
    "print(f\"Global feature vector calculation took {global_sim_end_time - global_sim_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "local_sim_time = time()\n",
    "\n",
    "point_feature_cameras = ['sceneshotcam', 'apple', 'milk', 'cereal', 'bread', 'banana', 'bin', 'table', 'ur5e', 'panda']\n",
    "specific_views = ['panda', 'cereal', 'milk', 'bread', 'apple', 'ur5e', 'banana', 'bin', 'table']#list(set(point_feature_cameras) - {\"sceneshotcam\"} - {f\"genpurp__{i}\" for i in range(1,6)})\n",
    "feat_per_obj = []\n",
    "obj_sim_per_unit_area = []\n",
    "for view in tqdm(specific_views):\n",
    "    obj_img = outputs[view][0] # rgb image\n",
    "    img_roi = torch.tensor(obj_img).permute(2, 0, 1).unsqueeze(0).float().to(gpu_device)\n",
    "    roifeat = visual_encoder(img_roi)\n",
    "    roifeat = roifeat.half().cuda()\n",
    "    roifeat = roifeat.mean(1)\n",
    "    roifeat = torch.nn.functional.normalize(roifeat, dim=-1)\n",
    "    feat_per_obj.append(roifeat)\n",
    "\n",
    "    # calculate the cosine similarity between the global feature vector and the feature vector for the object and save that as well\n",
    "    _sim = cosine_similarity(global_feat, roifeat)\n",
    "    obj_sim_per_unit_area.append(_sim)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "local_sim_end_time = time()\n",
    "print(f\"Local feature vector calculation took {local_sim_end_time - local_sim_time} seconds\")\n",
    "\n",
    "visual_encoder = None\n",
    "del visual_encoder\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_time = time()\n",
    "\n",
    "scores = torch.cat(obj_sim_per_unit_area).to(gpu_device)\n",
    "feat_per_obj = torch.cat(feat_per_obj, dim=0).to(gpu_device)\n",
    "\n",
    "# get the cosine simixlarity between the features of each object. This will be a square matrix where the (i, j)th entry is the cosine similarity between the ith and jth objects\n",
    "mask_sim_mat = torch.nn.functional.cosine_similarity(\n",
    "    feat_per_obj[:, :, None], feat_per_obj.t()[None, :, :]\n",
    ")\n",
    "mask_sim_mat_untouched = mask_sim_mat.clone()\n",
    "mask_sim_mat.fill_diagonal_(0.0) # set the diagonal to 0 because we don't want to consider the similarity between the same object\n",
    "mask_sim_mat = mask_sim_mat.mean(1)  # avg sim of each mask with each other mask\n",
    "softmax_scores = scores.cuda() - mask_sim_mat # subtracting the object-object relevance (which can be thought of as the relevance of the object in context of the other objects) object-scene similarity (which is kind of like global relevance) gives how much more or less important that object is than all the other objects\n",
    "softmax_scores = torch.nn.functional.softmax(softmax_scores, dim=0) # apply softmax to get the final scores\n",
    "\n",
    "score_end_time = time()\n",
    "print(f\"Score calculation took {score_end_time - score_time} seconds\")\n",
    "\n",
    "print(specific_views)\n",
    "print(softmax_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mask_sim_mat_untouched, \"figure_data/blip_noneva_rendfet_sim_square.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelwise_start_time = time()\n",
    "\n",
    "pixelwise_features = torch.zeros(outputs['xyz_pts'].shape[0], FEAT_DIM, dtype=torch.half)\n",
    "pixelwise_features = pixelwise_features.to(gpu_device)\n",
    "for objidx in range(len(specific_views)):\n",
    "    _weighted_feat = (\n",
    "        softmax_scores[objidx] * global_feat + (1 - softmax_scores[objidx]) * feat_per_obj[objidx]\n",
    "    )\n",
    "    _weighted_feat = torch.nn.functional.normalize(_weighted_feat, dim=-1)\n",
    "    pixelwise_features[outputs['segmentation_pts'][specific_views[objidx]], :] = _weighted_feat\n",
    "    # second normalization is unnecessary because all pixels in each object receive the same feature, and each point is only interacted with once.\n",
    "    # instead, we will normalize the features after voxel aggregation\n",
    "\n",
    "pixelwise_end_time = time()\n",
    "print(f\"Pixelwise feature calculation took {pixelwise_end_time - pixelwise_start_time} seconds\")\n",
    "\n",
    "print_mem_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It has been helpful to keep track of this metric to know when I've done something that messed up the feature extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_std = torch.std(pixelwise_features, dim=0)\n",
    "our_mean = torch.mean(pixelwise_features, dim=0)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "axs[0].hist(our_std.cpu().numpy(), bins=100, alpha=0.5, label=\"our std\")\n",
    "axs[0].legend()\n",
    "axs[1].hist(our_mean.cpu().numpy(), bins=100, alpha=0.5, label=\"our mean\")\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to save the pixelwise features as a tensor so that we can do the voxel grid sampling\n",
    "# unfortunately, that doesn't work with the version of pytorch that we use this notebook\n",
    "# WIP\n",
    "torch.save(pixelwise_features.cpu(), \"blip_noneva_pixelwise_rendfet_features.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "roco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
